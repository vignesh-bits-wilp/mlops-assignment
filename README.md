# MLOps Project - California Housing Prediction

A complete Machine Learning Operations (MLOps) project that demonstrates end-to-end ML pipeline development, including data ingestion, model training with MLflow tracking, model registry, and FastAPI deployment.

## ğŸ¯ **Assignment Progress**

| Part | Description | Status | Marks |
|------|-------------|--------|-------|
| **Part 1** | Repository and Data Versioning | âœ… Complete | 4/4 |
| **Part 2** | Model Development & Experiment Tracking | âœ… Complete | 6/6 |
| **Part 3** | API & Docker Packaging | âœ… Complete | 4/4 |
| **Part 4** | CI/CD with GitHub Actions | âœ… Complete | 6/6 |
| **Part 5** | Logging and Monitoring | âŒ Pending | 0/4 |
| **Part 6** | Summary + Demo | âŒ Pending | 0/2 |
| **Bonus** | Advanced Features | âŒ Pending | 0/4 |

**Total Score: 20/26 marks**

## ğŸ—ï¸ Project Structure

```
mlops-assignment/
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ data/              # Data processing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ models/            # Model training
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ train.py
â”‚   â”œâ”€â”€ api/               # FastAPI service
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ app.py
â”‚   â”œâ”€â”€ utils/             # Utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ data/                  # Data files
â”‚   â”œâ”€â”€ raw/              # Raw data files
â”‚   â”‚   â””â”€â”€ california_housing.csv
â”‚   â””â”€â”€ processed/        # Processed data files
â”‚       â””â”€â”€ cleaned.csv
â”œâ”€â”€ models/                # Trained models (future use)
â”œâ”€â”€ mlruns/                # MLflow tracking
â”œâ”€â”€ tests/                 # Tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_api.py
â”‚   â””â”€â”€ test_data_ingestion.py
â”œâ”€â”€ infra/                 # Infrastructure
â”œâ”€â”€ .github/workflows/     # CI/CD pipelines
â”‚   â””â”€â”€ ci.yml
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ download_data.py       # Data download script
â”œâ”€â”€ Dockerfile            # Docker configuration
â”œâ”€â”€ deploy.sh             # Deployment script
â”œâ”€â”€ pytest.ini           # Pytest configuration
â”œâ”€â”€ .flake8              # Flake8 configuration
â””â”€â”€ README.md             # This file
```

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.9+**
- **pip** package manager
- **Git** for version control
- **Docker** for containerization
- **DVC** for data version control (optional but recommended)

### 0. First-Time Setup (DVC Configuration)

If you're setting up the project for the first time, you'll need to initialize DVC for data version control:

```bash
# Install DVC (if not already installed)
pip install dvc

# Initialize DVC in the project
dvc init

# Add data files to DVC tracking
dvc add data/raw/california_housing.csv
dvc add data/processed/cleaned.csv

# Commit DVC files to Git
git add .dvc .dvcignore
git commit -m "Add data files to DVC tracking"
```

**Note:** If you don't want to use DVC, the data processing will still work, but you'll see warning messages about DVC not being configured.

### 1. Install Dependencies

```bash
# Install all required packages
pip install -r requirements.txt
```

### 2. Download Dataset

```bash
# Download California Housing dataset
python download_data.py
```

**Expected Output:**
```
Downloading California Housing dataset...
Dataset saved to: data/raw/california_housing.csv
Dataset shape: (20640, 9)
Features: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude', 'target']
```

### 3. Process Data

```bash
# Run data ingestion and cleaning
python src/data/data_ingestion.py
```

**Expected Output (with DVC configured):**
```
Loading raw data from data/raw/california_housing.csv...
Raw data shape: (20640, 9)
Cleaned data shape (after dropna): (20640, 9)
Cleaned data saved to data/processed/cleaned.csv
Adding data/raw/california_housing.csv to DVC tracking...
100% Adding...|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|1/1 [00:00<00:00, 13.19file/s]
Adding data/processed/cleaned.csv to DVC tracking...
100% Adding...|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|1/1 [00:00<00:00, 8.66file/s]
Data ingestion complete.
```

**Expected Output (without DVC):**
```
Loading raw data from data/raw/california_housing.csv...
Raw data shape: (20640, 9)
Cleaned data shape (after dropna): (20640, 9)
Cleaned data saved to data/processed/cleaned.csv
Warning: DVC tracking failed for data/raw/california_housing.csv
This is normal if the file is already tracked by Git or DVC is not configured.
Warning: DVC tracking failed for data/processed/cleaned.csv
This is normal if the file is already tracked by Git or DVC is not configured.
Data ingestion complete.
```

### 4. Train Models

```bash
# Train models with MLflow tracking
python src/models/train.py
```

**Expected Output:**
```
ğŸš€ Starting model training...
LinearRegression finished: RÂ² = 0.576
DecisionTreeRegressor finished: RÂ² = 0.600
Best run 6fc26fd8dea94875ac75cc76ba84441a with RÂ² = 0.600
Registered HousingModel version 9
Promoted version 9 to Production
âœ… Training & registration complete.
```

### 5. Start API Service

```bash
# Start FastAPI service
uvicorn src.api.app:app --host 127.0.0.1 --port 8000
```

**Expected Output:**
```
â–¶ Loading HousingModel version 9 (Production)
Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 988.62it/s]
INFO:     Started server process [20388]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
```

## ğŸ“Š Dataset Information

The California Housing dataset contains the following features:

| Feature | Description | Type |
|---------|-------------|------|
| MedInc | Median income in block group | float |
| HouseAge | Median house age in block group | float |
| AveRooms | Average number of rooms per household | float |
| AveBedrms | Average number of bedrooms per household | float |
| Population | Block group population | float |
| AveOccup | Average number of household members | float |
| Latitude | Block group latitude | float |
| Longitude | Block group longitude | float |
| target | Median house value (target variable) | float |

## ğŸ”§ API Usage

### Health Check

```bash
curl http://127.0.0.1:8000/health
```

**Response:**
```json
{"status": "ok"}
```

### Make Predictions

```bash
curl -X POST "http://127.0.0.1:8000/predict" \
     -H "Content-Type: application/json" \
     -d '{
       "MedInc": 8.3252,
       "HouseAge": 41.0,
       "AveRooms": 6.984127,
       "AveBedrms": 1.023810,
       "Population": 322.0,
       "AveOccup": 2.555556,
       "Latitude": 37.88,
       "Longitude": -122.23
     }'
```

**Response:**
```json
{"prediction": 4.526}
```

### Python Client Example

```python
import requests
import json

# Health check
response = requests.get("http://127.0.0.1:8000/health")
print(response.json())  # {"status": "ok"}

# Make prediction
features = {
    "MedInc": 8.3252,
    "HouseAge": 41.0,
    "AveRooms": 6.984127,
    "AveBedrms": 1.023810,
    "Population": 322.0,
    "AveOccup": 2.555556,
    "Latitude": 37.88,
    "Longitude": -122.23
}

response = requests.post(
    "http://127.0.0.1:8000/predict",
    json=features
)
prediction = response.json()
print(f"Predicted house value: ${prediction['prediction'] * 100000:.0f}")
```

## ğŸ§ª Model Performance

### Current Best Model: **DecisionTreeRegressor**
- **RÂ² Score**: 0.600
- **Features**: 8 numerical features
- **Target**: Median house value (California housing prices)
- **Training Time**: ~30 seconds
- **Model Size**: ~7MB

### Model Comparison

| Model | RÂ² Score | RMSE | Status |
|-------|----------|------|--------|
| LinearRegression | 0.576 | 0.847 | Baseline |
| DecisionTreeRegressor | 0.600 | 0.816 | **Production** |

## ğŸ” MLflow Tracking

View experiment tracking and model registry:

```bash
mlflow ui
```

This will start the MLflow UI at `http://localhost:5000`

### Available Experiments
- **housing_regression**: Main experiment tracking model training
- **Model Registry**: HousingModel with versioned deployments

## âš™ï¸ Part 4: CI/CD with GitHub Actions

This project includes a comprehensive CI/CD pipeline using GitHub Actions that meets all assignment requirements:

### **âœ… Assignment Requirements Met:**

1. **âœ… Lint/test code on push** - Automated code quality checks
2. **âœ… Build Docker image** - Containerized application
3. **âœ… Push to Docker Hub** - Image registry integration
4. **âœ… Deploy locally or to EC2** - Deployment automation

### **Workflow Overview**

The CI/CD pipeline (`.github/workflows/ci.yml`) includes:

1. **Lint & Test Job**:
   - Runs on every push to `master` and pull requests
   - Sets up Python 3.9
   - Installs dependencies with caching
   - Runs `flake8` for code linting
   - Runs `pytest` with coverage reporting
   - Uploads coverage to Codecov

2. **Build & Push Job**:
   - Runs only on successful pushes to `master`
   - Builds Docker image with proper tagging
   - Pushes to Docker Hub (requires secrets)

3. **Deploy Job**:
   - Simulates local deployment
   - Provides EC2 deployment instructions
   - Tests deployment functionality

### **Required Secrets**

To enable Docker image pushing, add these secrets to your GitHub repository:

- `DOCKERHUB_USERNAME`: Your Docker Hub username
- `DOCKERHUB_TOKEN`: Your Docker Hub access token

### **Local Testing**

Test the CI pipeline locally:

```bash
# Run linting
flake8 src tests

# Run tests with coverage
pytest tests/ -v --cov=src --cov-report=xml

# Run all tests
python -m pytest tests/ -v
```

### **Docker Image Tags**

The pipeline automatically tags Docker images with:
- `latest` for master branch
- `v1.0.0` for semantic version tags
- `master-abc123` for commit SHA tags
- `pr-123` for pull request tags

### **Deployment Options**

#### **Local Deployment:**
```bash
# Use the deployment script
chmod +x deploy.sh
./deploy.sh local

# Or manually
docker pull vignesh-bits-wilp/housing-api:latest
docker run -p 8000:8000 vignesh-bits-wilp/housing-api:latest
```

#### **EC2 Deployment:**
```bash
# Get deployment instructions
./deploy.sh ec2

# Or follow the manual steps:
# 1. SSH into EC2 instance
# 2. Install Docker
# 3. Pull and run the image
```

## ğŸ› ï¸ Development

### Adding New Models

1. **Edit `src/models/train.py`**
2. **Add your model to the `candidates` dictionary:**

```python
candidates = {
    "LinearRegression": LinearRegression(),
    "DecisionTreeRegressor": DecisionTreeRegressor(max_depth=5, random_state=42),
    "YourNewModel": YourNewModel(),  # Add here
}
```

3. **Run training to compare performance:**
```bash
python src/models/train.py
```

### Code Quality

```bash
# Format code
black .

# Lint code
flake8 .

# Sort imports
isort .
```

### Testing

```bash
# Run tests
python -m pytest tests/ -v

# Run with coverage
pytest tests/ -v --cov=src --cov-report=html
```

## ğŸ“ File Descriptions

| File | Purpose |
|------|---------|
| `download_data.py` | Downloads California Housing dataset |
| `src/data/data_ingestion.py` | Cleans and processes raw data |
| `src/models/train.py` | Trains models and registers with MLflow |
| `src/api/app.py` | FastAPI service for predictions |
| `src/utils/config.py` | Configuration settings |
| `tests/test_api.py` | API endpoint tests |
| `tests/test_data_ingestion.py` | Data processing tests |
| `.github/workflows/ci.yml` | CI/CD pipeline configuration |
| `deploy.sh` | Deployment automation script |
| `requirements.txt` | Python dependencies |

## ğŸš¨ Troubleshooting

### Common Issues

1. **DVC Not Configured**
   ```bash
   # Initialize DVC for data version control
   dvc init
   dvc add data/raw/california_housing.csv
   dvc add data/processed/cleaned.csv
   git add .dvc .dvcignore
   git commit -m "Add data files to DVC tracking"
   ```

2. **Port Already in Use**
   ```bash
   # Try different port
   uvicorn src.api.app:app --host 127.0.0.1 --port 8001
   ```

3. **MLflow Model Not Found**
   ```bash
   # Ensure mlruns directory exists
   ls mlruns/
   ```

4. **DVC Errors**
   - DVC errors are expected if not configured
   - Data processing will still work without DVC

5. **Module Import Errors**
   ```bash
   # Ensure you're in the project root
   python src/models/train.py
   ```

6. **CI/CD Pipeline Failures**
   ```bash
   # Test locally before pushing
   flake8 src tests
   pytest tests/ -v
   ```

7. **Docker Build Failures**
   ```bash
   # Check Dockerfile syntax
   docker build -t test-image .
   ```

### âœ… **Working Commands Summary**

| Step | Command | Status |
|------|---------|--------|
| DVC Setup | `dvc init && dvc add data/*` | âœ… Optional |
| Data Download | `python download_data.py` | âœ… Working |
| Data Processing | `python src/data/data_ingestion.py` | âœ… Working |
| Model Training | `python src/models/train.py` | âœ… Working |
| API Service | `uvicorn src.api.app:app --host 127.0.0.1 --port 8000` | âœ… Working |
| Testing | `pytest tests/ -v` | âœ… Working |
| Linting | `flake8 src tests` | âœ… Working |
| CI/CD Pipeline | GitHub Actions | âœ… Configured |
| Local Deployment | `./deploy.sh local` | âœ… Working |
| EC2 Deployment | `./deploy.sh ec2` | âœ… Instructions |

## ğŸ³ Docker Deployment

### Build the Docker Image

```bash
docker build -t mlops-service .
```

### Run the Container

```bash
docker run -p 8000:8000 mlops-service
```

The service will be available at `http://localhost:8000`

### CI/CD Docker Images

The GitHub Actions pipeline automatically builds and pushes Docker images to Docker Hub:

```bash
# Pull the latest image
docker pull vignesh-bits-wilp/housing-api:latest

# Run the containerized service
docker run -p 8000:8000 vignesh-bits-wilp/housing-api:latest
```

## ğŸ“ˆ Project Features

- âœ… **Data Pipeline**: Automated download and cleaning
- âœ… **Model Training**: MLflow-tracked experiments
- âœ… **Model Registry**: Versioned model management
- âœ… **API Service**: FastAPI prediction endpoint
- âœ… **Experiment Tracking**: MLflow integration
- âœ… **Production Deployment**: Model promotion workflow
- âœ… **Clean Architecture**: Well-organized source code structure
- âœ… **Data Version Control**: DVC integration (optional)
- âœ… **CI/CD Pipeline**: Automated testing and deployment
- âœ… **Code Quality**: Linting and testing automation
- âœ… **Docker Support**: Containerized deployment
- âœ… **Comprehensive Testing**: Unit tests with coverage
- âœ… **Deployment Automation**: Local and EC2 deployment scripts

## ğŸ¤ Contributing

1. Follow the existing code structure
2. Write tests for new functionality
3. Update documentation as needed
4. Ensure all components work together
5. Run the CI pipeline locally before pushing

## ğŸ“„ License

[Add your license here]
